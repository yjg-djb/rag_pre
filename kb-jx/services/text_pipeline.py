#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
文本清洗与去重管线 - 完整处理流程
"""
import re
from typing import List, Dict, Tuple, Optional
from utils.logger import get_logger
from utils.dedup_store import DedupStore, compute_sha256

# 安全导入 ftfy
try:
    import ftfy
    HAS_FTFY = True
except ImportError:
    HAS_FTFY = False

# 安全导入 simhash
try:
    from simhash import Simhash
    HAS_SIMHASH = True
except ImportError:
    HAS_SIMHASH = False

logger = get_logger("text_pipeline")


class TextPipeline:
    """文本清洗与去重管线"""
    
    def __init__(
        self,
        dedup_store: DedupStore,
        min_paragraph_len: int = 5,
        simhash_distance_threshold: int = 3,
        enable_near_duplicate: bool = True,
        noise_patterns: Optional[List[str]] = None,
        custom_noise_patterns: Optional[List[str]] = None,
        enable_cross_doc_dedup: bool = False
    ):
        """
        初始化文本管线
        
        Args:
            dedup_store: 去重存储后端
            min_paragraph_len: 最小段落长度（低于此长度的段落被过滤）
            simhash_distance_threshold: SimHash 汉明距离阈值（<=此值视为近重复）
            enable_near_duplicate: 是否启用近重复检测
            noise_patterns: 自定义噪声正则模式列表
            custom_noise_patterns: 额外的自定义噪声模式（会追加到 noise_patterns 后）
            enable_cross_doc_dedup: 是否启用跨文档段落去重（默认False）
        """
        self.dedup_store = dedup_store
        self.min_paragraph_len = min_paragraph_len
        self.simhash_distance_threshold = simhash_distance_threshold
        self.enable_near_duplicate = enable_near_duplicate and HAS_SIMHASH
        self.enable_cross_doc_dedup = enable_cross_doc_dedup
        
        # 噪声过滤模式（默认）
        self.noise_patterns = noise_patterns or [
            r'https?://\S+',  # URL
            r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b',  # Email
            r'\b\d{3}[-.\s]?\d{3,4}[-.\s]?\d{4}\b',  # 电话号码
            r'\b[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{4}-[0-9a-fA-F]{12}\b',  # GUID
            r'\b[0-9a-fA-F]{32,64}\b',  # 哈希串
            r'第\s*\d+\s*页',  # 页码标记
            r'Page\s+\d+',  # 页码标记（英文）
            r'\d{4}[-/]\d{1,2}[-/]\d{1,2}\s+\d{1,2}:\d{2}(:\d{2})?',  # 时间戳
            r'[-=_]{5,}',  # 重复分隔符
            r'Generated by \w+',  # 生成器标记
            r'Printed on \d{4}-\d{2}-\d{2}',  # 打印日期
        ]
        
        # 追加自定义噪声模式
        if custom_noise_patterns:
            self.noise_patterns.extend(custom_noise_patterns)
        
        if not HAS_FTFY:
            logger.warning("ftfy 未安装，Unicode 修复功能不可用")
        if not HAS_SIMHASH:
            logger.warning("simhash 未安装，近重复检测功能不可用")
            self.enable_near_duplicate = False
    
    def process(self, text: str, doc_name: str = "unknown") -> Dict:
        """
        完整处理流程
        
        Args:
            text: 原始文本
            doc_name: 文档名称（用于日志）
        
        Returns:
            {
                "success": bool,
                "cleaned_text": str,
                "stats": {
                    "original_length": int,
                    "normalized_length": int,
                    "noise_removed_count": int,
                    "paragraphs_original": int,
                    "paragraphs_after_dedup": int,
                    "paragraphs_exact_dup": int,
                    "paragraphs_near_dup": int,
                    "paragraphs_too_short": int,
                    "final_length": int
                },
                "doc_duplicate": bool,
                "message": str
            }
        """
        logger.info(f"[{doc_name}] 开始文本清洗与去重管线")
        
        stats = {
            "original_length": len(text),
            "normalized_length": 0,
            "noise_removed_count": 0,
            "paragraphs_original": 0,
            "paragraphs_after_dedup": 0,
            "paragraphs_exact_dup": 0,
            "paragraphs_near_dup": 0,
            "paragraphs_too_short": 0,
            "final_length": 0
        }
        
        # 步骤1: 文档级去重检查（先检查，但不影响清洗流程）
        doc_hash = compute_sha256(text)
        is_doc_duplicate = self.dedup_store.is_doc_seen(doc_hash)
        
        if is_doc_duplicate:
            logger.info(f"[{doc_name}] 文档级去重命中: {doc_hash[:16]}...，但仍执行清洗流程以便后续使用")
        
        # 步骤2: 文本规范化
        normalized = self._normalize_text(text)
        stats["normalized_length"] = len(normalized)
        logger.debug(f"[{doc_name}] 规范化完成: {stats['original_length']} -> {stats['normalized_length']} 字符")
        
        # 步骤3: 噪声过滤
        cleaned, noise_count = self._noise_filter(normalized)
        stats["noise_removed_count"] = noise_count
        logger.debug(f"[{doc_name}] 噪声过滤完成: 移除 {noise_count} 处噪声")
        
        # 步骤4: 段落拆分
        paragraphs = self._split_paragraphs(cleaned)
        stats["paragraphs_original"] = len(paragraphs)
        logger.debug(f"[{doc_name}] 段落拆分完成: {len(paragraphs)} 个段落")
        
        # 步骤5: 段落去重（精确 + 近重复）
        deduped_paras, exact_dup, near_dup, too_short = self._dedup_paragraphs(paragraphs, doc_name)
        stats["paragraphs_after_dedup"] = len(deduped_paras)
        stats["paragraphs_exact_dup"] = exact_dup
        stats["paragraphs_near_dup"] = near_dup
        stats["paragraphs_too_short"] = too_short
        logger.info(f"[{doc_name}] 段落去重完成: {len(paragraphs)} -> {len(deduped_paras)} (精确重复:{exact_dup}, 近重复:{near_dup}, 过短:{too_short})")
        
        # 步骤6: 格式标准化
        standardized_paras = self._format_standardize(deduped_paras)
        
        # 步骤7: 组装最终文本
        final_text = self._assemble(standardized_paras)
        stats["final_length"] = len(final_text)
        
        # 标记文档已处理（避免后续重复）
        if not is_doc_duplicate:
            self.dedup_store.mark_doc(doc_hash)
        
        logger.info(f"[{doc_name}] 管线完成: {stats['original_length']} -> {stats['final_length']} 字符, {stats['paragraphs_after_dedup']} 段落")
        
        # 如果是去重文档，success=False 但返回清洗后的文本
        if is_doc_duplicate:
            return {
                "success": False,
                "cleaned_text": final_text,
                "stats": stats,
                "doc_duplicate": True,
                "message": "文档已存在（完全重复）但已清洗"
            }
        
        return {
            "success": True,
            "cleaned_text": final_text,
            "stats": stats,
            "doc_duplicate": False,
            "message": "处理成功"
        }
    
    def _normalize_text(self, text: str) -> str:
        """
        文本规范化
        - 修复 Unicode 错误
        - 去除 BOM
        - 去除零宽字符
        - 标准化空白（制表符转空格、连续空格折叠）
        """
        # 1. 使用 ftfy 修复 Unicode
        if HAS_FTFY:
            text = ftfy.fix_text(text)
        
        # 2. 去除 BOM
        text = text.replace('\ufeff', '')
        
        # 3. 去除零宽字符
        zero_width_chars = [
            '\u200b',  # 零宽空格
            '\u200c',  # 零宽非连接符
            '\u200d',  # 零宽连接符
            '\ufeff',  # 零宽非断空格
        ]
        for char in zero_width_chars:
            text = text.replace(char, '')
        
        # 4. 去除不可见控制字符（保留换行、制表符）
        text = ''.join(c for c in text if c.isprintable() or c in '\n\r\t ')
        
        # 5. 制表符转空格
        text = text.replace('\t', '    ')
        
        # 6. 统一换行符
        text = text.replace('\r\n', '\n').replace('\r', '\n')
        
        # 7. 折叠连续空格（保留换行）
        text = re.sub(r' {2,}', ' ', text)
        
        # 8. 折叠连续空行（保留双换行作为段落分隔）
        text = re.sub(r'\n{4,}', '\n\n\n', text)
        
        return text
    
    def _noise_filter(self, text: str) -> Tuple[str, int]:
        """
        噪声过滤
        
        Returns:
            (清洗后文本, 移除次数)
        """
        removed_count = 0
        
        for pattern in self.noise_patterns:
            matches = re.findall(pattern, text)
            if matches:
                removed_count += len(matches)
                text = re.sub(pattern, '', text)
                logger.debug(f"噪声模式 '{pattern[:30]}...' 移除 {len(matches)} 处")
        
        return text, removed_count
    
    def _split_paragraphs(self, text: str) -> List[str]:
        """
        段落拆分
        - 按双换行（或更多）拆分
        - 去除首尾空白
        - 过滤空段落
        """
        # 按双换行拆分
        paragraphs = re.split(r'\n{2,}', text)
        
        # 清理每个段落
        cleaned = []
        for para in paragraphs:
            para = para.strip()
            if para:  # 非空
                cleaned.append(para)
        
        return cleaned
    
    def _dedup_paragraphs(self, paragraphs: List[str], doc_name: str) -> Tuple[List[str], int, int, int]:
        """
        段落去重（精确 + 近重复）
        
        Returns:
            (去重后段落, 精确重复数, 近重复数, 过短数)
        """
        result = []
        exact_dup_count = 0
        near_dup_count = 0
        too_short_count = 0
        
        # 获取已存在的 SimHash（用于跨文档去重）
        if self.enable_cross_doc_dedup:
            # 启用跨文档去重：从全局存储获取
            existing_simhash = self.dedup_store.get_all_para_simhash() if self.enable_near_duplicate else {}
        else:
            # 禁用跨文档去重：只在本文档内去重
            existing_simhash = {}
        
        # 本文档内的段落哈希（用于文档内去重）
        local_para_hashes = set()
        
        for i, para in enumerate(paragraphs):
            # 过滤过短段落
            if len(para) < self.min_paragraph_len:
                too_short_count += 1
                logger.debug(f"[{doc_name}] 段落 {i+1} 过短({len(para)}字符)，跳过")
                continue
            
            # 精确去重（SHA256）
            para_hash = compute_sha256(para)
            
            # 文档内精确去重（必须）
            if para_hash in local_para_hashes:
                exact_dup_count += 1
                logger.debug(f"[{doc_name}] 段落 {i+1} 文档内精确重复，跳过")
                continue
            
            # 跨文档精确去重（可选）
            if self.enable_cross_doc_dedup and self.dedup_store.is_para_seen(para_hash):
                exact_dup_count += 1
                logger.debug(f"[{doc_name}] 段落 {i+1} 跨文档精确重复，跳过")
                continue
            
            # 近重复检测（SimHash）
            is_near_dup = False
            para_simhash = None
            
            if self.enable_near_duplicate:
                para_simhash = Simhash(para).value
                
                # 与已存在的段落比对（只在启用跨文档去重时）
                if self.enable_cross_doc_dedup:
                    for existing_hash, existing_sim in existing_simhash.items():
                        distance = self._hamming_distance(para_simhash, existing_sim)
                        if distance <= self.simhash_distance_threshold:
                            near_dup_count += 1
                            is_near_dup = True
                            logger.debug(f"[{doc_name}] 段落 {i+1} 跨文档近重复(距离={distance})，跳过")
                            break
            
            if is_near_dup:
                continue
            
            # 保留段落
            result.append(para)
            
            # 记录到本地集合（文档内去重）
            local_para_hashes.add(para_hash)
            
            # 记录到全局存储（只在启用跨文档去重时）
            if self.enable_cross_doc_dedup:
                self.dedup_store.mark_para(para_hash, para_simhash)
            
            # 更新本次处理的 simhash 集合（避免同文档内重复）
            if para_simhash is not None and self.enable_cross_doc_dedup:
                existing_simhash[para_hash] = para_simhash
        
        return result, exact_dup_count, near_dup_count, too_short_count
    
    def _hamming_distance(self, hash1: int, hash2: int) -> int:
        """计算两个整数的汉明距离（二进制位不同的个数）"""
        x = hash1 ^ hash2
        distance = 0
        while x:
            distance += 1
            x &= x - 1  # 清除最右边的 1
        return distance
    
    def _format_standardize(self, paragraphs: List[str]) -> List[str]:
        """
        格式标准化
        - 每段首尾 trim
        - 段内连续空格折叠
        - 统一列表符号（可选）
        """
        standardized = []
        
        for para in paragraphs:
            # 首尾 trim
            para = para.strip()
            
            # 段内连续空格折叠（保留单个空格）
            para = re.sub(r' {2,}', ' ', para)
            
            # 统一列表符号（可选，这里示例统一为 "- "）
            # para = re.sub(r'^[•·●○]\s*', '- ', para)
            
            standardized.append(para)
        
        return standardized
    
    def _assemble(self, paragraphs: List[str]) -> str:
        """
        组装最终文本
        - 段落间用双换行分隔
        """
        return '\n\n'.join(paragraphs)
